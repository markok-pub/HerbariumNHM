{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc06aa8-d8d6-4927-814b-300009a0abe7",
   "metadata": {},
   "source": [
    "# Classification of the feature vectors acquired from the Contrastive Learning model\n",
    "## - Linear Classification\n",
    "## - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47ad4f-86bf-4198-ad06-5ee45324b6ce",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ecedb2e-c7de-4d82-a990-bace82d1b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_523567/1609014514.py:15: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "#import skimage\n",
    "import pandas\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import bokeh\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.models as models\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "import cv2\n",
    "import pathlib\n",
    "import os\n",
    "import datetime\n",
    "#import tensorflow as tf\n",
    "\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e70ee-f636-4284-bbae-cedbe04302ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipywidgets==7.7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166a6cc-5de2-4b36-8160-5a00d183a1ba",
   "metadata": {},
   "source": [
    "### 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e8e79bb-10a3-44e2-b185-8c260d60f037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679989\n",
      "210408\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "data_path_train = \"/gpfs/data/fs71186/kadic/train_images\"\n",
    "data_path_test = \"/gpfs/data/fs71186/kadic/test_images\"\n",
    "\n",
    "ground_truths = \"/gpfs/data/fs71186/kadic/train_metadata.json\"\n",
    "\n",
    "train_image_files = [join(dirpath,f) for (dirpath, dirnames, filenames) in walk(data_path_train) for f in filenames] \n",
    "\n",
    "print(len(train_image_files))\n",
    "\n",
    "test_image_files = [join(dirpath,f) for (dirpath, dirnames, filenames) in walk(data_path_test) for f in filenames] \n",
    "\n",
    "print(len(test_image_files))\n",
    "\n",
    "train_image_files = sorted(train_image_files)\n",
    "test_image_files = sorted(test_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b17354a-29e3-4272-b401-afaaaf0adabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['annotations', 'images', 'categories', 'genera', 'institutions', 'distances', 'license'])\n",
      "839772\n",
      "{'genus_id': 1, 'institution_id': 0, 'category_id': 0, 'image_id': '00000__001'}\n",
      "/gpfs/data/fs71186/kadic/train_images/000/00/00000__001.jpg\n",
      "15501\n",
      "2564\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open(ground_truths)\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "ground_truth_data = json.load(f)\n",
    "\n",
    "print(ground_truth_data.keys())\n",
    "print(len(ground_truth_data[\"annotations\"]))\n",
    "print(ground_truth_data[\"annotations\"][0])\n",
    "print(train_image_files[0])\n",
    "\n",
    "print(len(ground_truth_data[\"categories\"]))\n",
    "print(len(ground_truth_data[\"genera\"]))\n",
    "      \n",
    "gt_annot = ground_truth_data[\"annotations\"]\n",
    "# Iterating through the json\n",
    "# list \n",
    "#Closing file\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acad1f00-81a8-4ef9-b4fc-f510fdd87635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/data/fs71186/kadic/train_images/000/00/00000__001.jpg\n",
      "00000__001\n",
      "/gpfs/data/fs71186/kadic/train_images/019/33/01933__009.jpg\n",
      "01933__009\n",
      "/gpfs/data/fs71186/kadic/train_images/037/74/03774__113.jpg\n",
      "03774__113\n",
      "/gpfs/data/fs71186/kadic/train_images/055/74/05574__051.jpg\n",
      "05574__051\n",
      "/gpfs/data/fs71186/kadic/train_images/074/12/07412__018.jpg\n",
      "07412__018\n",
      "/gpfs/data/fs71186/kadic/train_images/092/23/09223__036.jpg\n",
      "09223__036\n",
      "/gpfs/data/fs71186/kadic/train_images/111/17/11117__031.jpg\n",
      "11117__031\n",
      "679989\n",
      "679989\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i, img in enumerate(train_image_files):\n",
    "    if(i % 100000 == 0):\n",
    "        #print(i)\n",
    "        print(img)\n",
    "        print(gt_annot[i]['image_id'])\n",
    "    train_data.append((img, gt_annot[i]['category_id']))\n",
    "\n",
    "print(len(train_image_files))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f46c38b-46d2-4ab4-b557-8ccf1861d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "label_count = {}\n",
    "for img, annot in train_data:\n",
    "    if annot not in labels:\n",
    "        labels.append(annot)\n",
    "        label_count[str(annot)] = 1\n",
    "    else:\n",
    "        label_count[str(annot)] = int(label_count[str(annot)]) + 1\n",
    "        \n",
    "#print(len(labels))\n",
    "#print(label_count)\n",
    "sorted_count = dict(sorted(label_count.items(), key=lambda item: item[1]))\n",
    "\n",
    "#print(list(sorted_count.items())[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b5e4baf-8350-4296-8dcb-88b6c3b5da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12533\n",
      "200\n",
      "15965\n"
     ]
    }
   ],
   "source": [
    "print(len(list(sorted_count.items())))\n",
    "biggest_categories = list(sorted_count.items())[12333:]\n",
    "#print(list(sorted_count.items())[12333:])\n",
    "\n",
    "print(len(biggest_categories))\n",
    "cat_sum = 0\n",
    "for cat in biggest_categories:\n",
    "    cat_sum += cat[1]\n",
    "print(cat_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc7b7e30-d695-4e93-b18b-02bb59bc1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys_back = {}\n",
    "for i, cat in enumerate(biggest_categories):\n",
    "    dict_keys_back[str(cat[0])] = i\n",
    "#print(dict_keys_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7258cc9f-4f0f-4004-abbd-617c98884efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15965\n"
     ]
    }
   ],
   "source": [
    "reduced_train_data = []\n",
    "for img, annot in train_data:\n",
    "    for key, val in biggest_categories:\n",
    "        if str(annot) == str(key):\n",
    "            reduced_train_data.append((img, annot))\n",
    "\n",
    "print(len(reduced_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1457939-856a-4d8a-85f2-c8595300b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15965\n"
     ]
    }
   ],
   "source": [
    "reduced_train_data_subbed = []\n",
    "for img, annot in train_data:\n",
    "    for key, val in biggest_categories:\n",
    "        if str(annot) == str(key):\n",
    "            reduced_train_data_subbed.append((img, dict_keys_back[str(annot)]))\n",
    "\n",
    "print(len(reduced_train_data_subbed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c55b6c0-4fb1-44d6-ad95-369a26e2f650",
   "metadata": {},
   "source": [
    "### 3. Parameters for loading the SimCLR model and for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe87affe-a971-4b14-81f5-9b68e4646361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of workers: 128\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = data_path_train\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/contrastive_models\"\n",
    "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
    "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
    "NUM_WORKERS = 128 #os.cpu_count()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73fe9705-9695-4708-9187-eab3e2ce1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths,transform):\n",
    "        self.paths = [i[0] for i in paths]\n",
    "        self.transform = transform\n",
    "        self.target_paths = [i[1] for i in paths]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.paths[index]\n",
    "        image_l = Image.open(image_path)\n",
    "        image = image_l.convert('RGB')\n",
    "        image_tensor = image\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        \n",
    "        target = self.target_paths[index]\n",
    "        \n",
    "        return (image_tensor, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859573fa-5f77-4572-bd93-0097371c6458",
   "metadata": {},
   "source": [
    "### 4. Load The Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e950929-1427-4cbd-a4d2-4da265ff119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear\n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        imgs, _ = batch\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs)\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # Logging loss\n",
    "        self.log(mode+'_loss', nll)\n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)],\n",
    "                             dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc39550-aceb-4b52-80ff-66d0305b81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.SimCLR'>\n"
     ]
    }
   ],
   "source": [
    "pretrained_filename = \"../saved_models/contrastive_models/SimCLR_Eval_2_Validated.ckpt\"\n",
    "\n",
    "simclr_model = SimCLR.load_from_checkpoint(pretrained_filename)\n",
    "print(type(simclr_model))\n",
    "#train_feats_simclr = prepare_data_features(simclr_model, dataset_full_nocut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2ce0c-33a4-4e0b-8c92-148316800ce5",
   "metadata": {},
   "source": [
    "## 5. Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00180d33-02b0-430d-a8b1-dc866fe65cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, feature_dim, num_classes, lr, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # Mapping from representation h to classes\n",
    "        self.model = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs*0.6),\n",
    "                                                                  int(self.hparams.max_epochs*0.8)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        feats, labels = batch\n",
    "        preds = self.model(feats)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07820a5a-8538-4f44-b144-6138845a0cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eaa5bd-5314-438c-961e-250e8fbe1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([transforms.Resize((1000, 666)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))])\n",
    "cpy = reduced_train_data\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(cpy)\n",
    "\n",
    "tr_files = cpy[0:12772] #train_data[0:543991]\n",
    "ts_files = cpy[12772:15965] #train_data[543991:]\n",
    "\n",
    "print(len(tr_files))\n",
    "\n",
    "for file in ts_files:\n",
    "    print(file[1])\n",
    "\n",
    "train_img_data = ImageDataset(tr_files, transform = img_transforms)\n",
    "\n",
    "test_img_data = ImageDataset(ts_files, transform = img_transforms)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_img_data))\n",
    "print(\"Number of test examples:\", len(test_img_data))\n",
    "\n",
    "print(type(train_img_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f883fdbc-deb9-4689-8d69-b05f5c074700",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prepare_data_features(model, dataset):\n",
    "    # Prepare model\n",
    "    network = deepcopy(model.convnet)\n",
    "    network.fc = nn.Identity()  # Removing projection head g(.)\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    # Encode all images\n",
    "    data_loader = data.DataLoader(dataset, batch_size=64, num_workers=NUM_WORKERS, shuffle=False, drop_last=False)\n",
    "    feats, labels = [], []\n",
    "    for batch_imgs, batch_labels in tqdm(data_loader):\n",
    "        #print(len(batch_imgs))\n",
    "        batch_imgs = batch_imgs.to(device)\n",
    "        batch_feats = network(batch_imgs)\n",
    "        feats.append(batch_feats.detach().cpu())\n",
    "        labels.append(batch_labels)\n",
    "\n",
    "    feats = torch.cat(feats, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "\n",
    "    # Sort images by labels\n",
    "    labels, idxs = labels.sort()\n",
    "    feats = feats[idxs]\n",
    "\n",
    "    return data.TensorDataset(feats, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "065a887a-5841-46dd-a956-f0058182d55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca2453807ee477c9db6ae2bc0c449b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8187de4be7fb4504a5bb02a41d3558dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_feats_simclr = prepare_data_features(simclr_model, train_img_data)\n",
    "test_feats_simclr = prepare_data_features(simclr_model, test_img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "110ea364-a9db-4734-9452-12c74bef9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(batch_size, train_feats_data, test_feats_data, model_suffix, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"LogisticRegression\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         enable_progress_bar=False,\n",
    "                         check_val_every_n_epoch=10, \n",
    "                            )\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = data.DataLoader(train_feats_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = data.DataLoader(test_feats_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"LogisticRegression_{model_suffix}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = LogisticRegression.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)  # To be reproducable\n",
    "        model = LogisticRegression(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = LogisticRegression.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on train and validation set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": test_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20fe84-0c58-40cc-b818-e38a76abd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, set_results = train_logreg(batch_size=64,\n",
    "                                        train_feats_data=train_feats_simclr,\n",
    "                                        test_feats_data=test_feats_simclr,\n",
    "                                        model_suffix=\"first_one\",\n",
    "                                        feature_dim=train_feats_simclr.tensors[0].shape[1],\n",
    "                                        num_classes=200,\n",
    "                                        lr=1e-3,\n",
    "                                        weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cf718-a478-4193-921b-fbfa11510202",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 15000\n",
    "test_scores = results[\"test\"]\n",
    "\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc7edf-857f-4a5a-b641-9a48b868a9cd",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machine Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9959d1-c221-4b2a-a1a0-fd5cca725bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
