{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e612a20e-e074-4f0e-95b8-c8145d217619",
   "metadata": {},
   "source": [
    "# ResNet18 Classification on the Herbarium 2022 Dataset\n",
    "## - To serve as baseline for the evaluation of Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4185aed-c5a8-4792-bc94-50ff0f96db25",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fb80e-42d6-4c27-9cf9-da20257bf472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "#import skimage\n",
    "import pandas\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import bokeh\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torchvision.models as models\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "import cv2\n",
    "import pathlib\n",
    "import os\n",
    "import datetime\n",
    "#import tensorflow as tf\n",
    "\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb8054-a053-4a3d-a9aa-858814ed6a6f",
   "metadata": {},
   "source": [
    "### 2. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d96129-24e8-4c40-b7ab-fe49a8a3b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "data_path_train = \"/gpfs/data/fs71186/kadic/train_images\"\n",
    "data_path_test = \"/gpfs/data/fs71186/kadic/test_images\"\n",
    "\n",
    "ground_truths = \"/gpfs/data/fs71186/kadic/train_metadata.json\"\n",
    "\n",
    "train_image_files = [join(dirpath,f) for (dirpath, dirnames, filenames) in walk(data_path_train) for f in filenames] \n",
    "\n",
    "print(len(train_image_files))\n",
    "\n",
    "test_image_files = [join(dirpath,f) for (dirpath, dirnames, filenames) in walk(data_path_test) for f in filenames] \n",
    "\n",
    "print(len(test_image_files))\n",
    "\n",
    "train_image_files = sorted(train_image_files)\n",
    "test_image_files = sorted(test_image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d693dd9-f937-4221-a627-ca910a91955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open(ground_truths)\n",
    " \n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "ground_truth_data = json.load(f)\n",
    "\n",
    "print(ground_truth_data.keys())\n",
    "print(len(ground_truth_data[\"annotations\"]))\n",
    "print(ground_truth_data[\"annotations\"][0])\n",
    "print(train_image_files[0])\n",
    "\n",
    "print(len(ground_truth_data[\"categories\"]))\n",
    "print(len(ground_truth_data[\"genera\"]))\n",
    "      \n",
    "gt_annot = ground_truth_data[\"annotations\"]\n",
    "# Iterating through the json\n",
    "# list \n",
    "#Closing file\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a7962-24b0-448a-abbd-cf2af9840004",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i, img in enumerate(train_image_files):\n",
    "    if(i % 100000 == 0):\n",
    "        #print(i)\n",
    "        print(img)\n",
    "        print(gt_annot[i]['image_id'])\n",
    "    train_data.append((img, gt_annot[i]['category_id']))\n",
    "\n",
    "print(len(train_image_files))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33b804-6fd9-434b-8389-59a6cb064142",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "label_count = {}\n",
    "for img, annot in train_data:\n",
    "    if annot not in labels:\n",
    "        labels.append(annot)\n",
    "        label_count[str(annot)] = 1\n",
    "    else:\n",
    "        label_count[str(annot)] = int(label_count[str(annot)]) + 1\n",
    "        \n",
    "#print(len(labels))\n",
    "#print(label_count)\n",
    "sorted_count = dict(sorted(label_count.items(), key=lambda item: item[1]))\n",
    "\n",
    "#print(list(sorted_count.items())[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5fb66b-fe26-4f25-9756-0f7a8afec6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(sorted_count.items())))\n",
    "biggest_categories = list(sorted_count.items())[12333:]\n",
    "#print(list(sorted_count.items())[12333:])\n",
    "\n",
    "print(len(biggest_categories))\n",
    "cat_sum = 0\n",
    "for cat in biggest_categories:\n",
    "    cat_sum += cat[1]\n",
    "print(cat_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9151e-56d5-4709-a89c-56440c741a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys_back = {}\n",
    "for i, cat in enumerate(biggest_categories):\n",
    "    dict_keys_back[str(cat[0])] = i\n",
    "#print(dict_keys_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7959bc-beae-4b0c-b36d-108c7c405a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train_data = []\n",
    "for img, annot in train_data:\n",
    "    for key, val in biggest_categories:\n",
    "        if str(annot) == str(key):\n",
    "            reduced_train_data.append((img, annot))\n",
    "\n",
    "print(len(reduced_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5a64a-4f6c-4413-8439-7204cf63f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train_data_subbed = []\n",
    "for img, annot in train_data:\n",
    "    for key, val in biggest_categories:\n",
    "        if str(annot) == str(key):\n",
    "            reduced_train_data_subbed.append((img, dict_keys_back[str(annot)]))\n",
    "\n",
    "print(len(reduced_train_data_subbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4cb2f-5b6d-4aaa-9ece-3c4f600a25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths,transform):\n",
    "        self.paths = [i[0] for i in paths]\n",
    "        self.transform = transform\n",
    "        self.target_paths = [i[1] for i in paths]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.paths[index]\n",
    "        image_l = Image.open(image_path)\n",
    "        image = image_l.convert('RGB')\n",
    "        image_tensor = image\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "        \n",
    "        target = self.target_paths[index]\n",
    "        \n",
    "        return (image_tensor, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ff6c59-21ec-4b51-9cfc-a9711d319e0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img_transforms \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m666\u001b[39m)),\n\u001b[1;32m      2\u001b[0m                                      transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      3\u001b[0m                                      transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#ImageDataset(train_image_files, \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#                       transform=img_transforms)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tr_files \u001b[38;5;241m=\u001b[39m reduced_train_data_subbed[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m12772\u001b[39m] \u001b[38;5;66;03m#train_data[0:543991]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "tr_files = reduced_train_data_subbed[0:12772] #train_data[0:543991]\n",
    "ts_files = reduced_train_data_subbed[12772:15965] #train_data[543991:]\n",
    "\n",
    "print(len(tr_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fede35e-1860-49f4-ab49-b73822742b01",
   "metadata": {},
   "source": [
    "### 3. ResNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdfcaf-cdc9-4cca-9213-438bc097f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = data_path_train\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/contrastive_models\"\n",
    "# In this notebook, we use data loaders with heavier computational processing. It is recommended to use as many\n",
    "# workers as possible in a data loader, which corresponds to the number of CPU cores\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Number of workers:\", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbce06-9f4e-4834-98a9-9f15d81d6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes, lr, weight_decay, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = torchvision.models.resnet18(num_classes=num_classes)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(),\n",
    "                                lr=self.hparams.lr,\n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                      milestones=[int(self.hparams.max_epochs*0.7),\n",
    "                                                                  int(self.hparams.max_epochs*0.9)],\n",
    "                                                      gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(mode + '_loss', loss)\n",
    "        self.log(mode + '_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb1524e-02a0-4cfd-9943-e9e27e1c6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomResizedCrop(size=96, scale=(0.8, 1.0)),\n",
    "                                       transforms.RandomGrayscale(p=0.2),\n",
    "                                       transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 0.5)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,), (0.5,))\n",
    "                                       ])\n",
    "train_img_data = ImageDataset(tr_files, transform = train_transforms)\n",
    "\n",
    "img_transforms = transforms.Compose([transforms.Resize((1000, 666)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "test_img_data = ImageDataset(ts_files, transform = img_transforms)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_img_data))\n",
    "print(\"Number of test examples:\", len(test_img_data))\n",
    "\n",
    "print(type(train_img_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b3bfb-5e7d-47e2-a6bb-227bba306261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(batch_size, max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ResNet\"),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         check_val_every_n_epoch=2)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = data.DataLoader(train_img_data, batch_size=batch_size, shuffle=True,\n",
    "                                   drop_last=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    test_loader = data.DataLoader(test_img_data, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ResNet.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        model = ResNet.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ResNet(**kwargs)\n",
    "        trainer.fit(model, train_loader, test_loader)\n",
    "        model = ResNet.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation set\n",
    "    train_result = trainer.test(model, train_loader, verbose=False)\n",
    "    val_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"train\": train_result[0][\"test_acc\"], \"test\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbcc77-93d0-4a65-9291-db2c845b4b6e",
   "metadata": {},
   "source": [
    "### 4. Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a964046-4e3f-4330-953f-3e1fe5092d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model, resnet_result = train_resnet(batch_size=64,\n",
    "                                           num_classes=200,\n",
    "                                           lr=1e-3,\n",
    "                                           weight_decay=2e-4,\n",
    "                                           max_epochs=100)\n",
    "print(f\"Accuracy on training set: {100*resnet_result['train']:4.2f}%\")\n",
    "print(f\"Accuracy on test set: {100*resnet_result['test']:4.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
